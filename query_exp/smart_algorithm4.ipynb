{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smart using Rocchio's algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\liamg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\liamg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import math\n",
    "import json\n",
    "import ast\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.stem.snowball import FrenchStemmer, GermanStemmer, ItalianStemmer, SpanishStemmer, EnglishStemmer, ArabicStemmer\n",
    "from kiwipiepy import Kiwi\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "from spacy.lang.ko.stop_words import STOP_WORDS as ko_stop\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Access corpus and queries of respective languages when already tokenized and preprocessed previously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = 'fr'\n",
    "queries_df = pd.read_csv(f'Data_query_exp/{lang}_query.csv') \n",
    "corpus_df = pd.read_csv(f'Data_query_exp/{lang}_corpus.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the text needs to be retrieved and tokenized, then the cells below are run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = 'fr'\n",
    "with open(\"Data/corpus.json/corpus.json\", 'r', encoding='utf-8') as f:\n",
    "    corpus_data = json.load(f)\n",
    "\n",
    "corpus_df = pd.DataFrame(corpus_data)\n",
    "queries_df = pd.read_csv('Data/dev.csv')\n",
    "corpus_df = corpus_df[corpus_df['lang']==lang]\n",
    "queries_df = queries_df[queries_df['lang']==lang]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if lang == 'en':\n",
    "    stemmer = EnglishStemmer()\n",
    "    stopwords = stopwords.words(\"english\")\n",
    "elif lang == 'fr':\n",
    "    stemmer = FrenchStemmer()\n",
    "    stopwords = stopwords.words(\"french\")\n",
    "elif lang == 'de':\n",
    "    stemmer = GermanStemmer()\n",
    "    stopwords = stopwords.words(\"german\")\n",
    "elif lang == 'it':\n",
    "    stemmer = ItalianStemmer()\n",
    "    stopwords = stopwords.words(\"italian\")\n",
    "elif lang == 'es':\n",
    "    stemmer = SpanishStemmer()\n",
    "    stopwords = stopwords.words(\"spanish\")\n",
    "elif lang == 'ar':\n",
    "    stemmer = ArabicStemmer()\n",
    "    stopwords = stopwords.words(\"arabic\")\n",
    "elif lang == 'ko':\n",
    "    stemmer = Kiwi()\n",
    "    stopwords = list(ko_stop)\n",
    "\n",
    "def tokenize(text):\n",
    "    text = text.translate(str.maketrans(string.punctuation, ' ' * len(string.punctuation)))\n",
    "\n",
    "    if lang != 'ko':\n",
    "        tokens = word_tokenize(text) \n",
    "        tokenized_words = [stemmer.stem(word.lower()) for word in tokens if word.lower() not in stopwords]\n",
    "    else:\n",
    "        tokens = stemmer.analyze(text)[0][0] \n",
    "        tokenized_words = [word.form for word in tokens if word.tag.startswith('N') and word.form not in stopwords] \n",
    "    \n",
    "    return tokenized_words\n",
    "\n",
    "corpus_df['text_token'] = corpus_df['text'].apply(tokenize)\n",
    "queries_df['query_token'] = queries_df['query'].apply(tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorize the documents and the queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(max_features=7500)\n",
    "corpus_data_matrix = vectorizer.fit_transform(corpus_df['text_token'])\n",
    "query_matrix = vectorizer.transform(queries_df['query_token'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate term frequency and inverse document frequency matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_matrix = corpus_data_matrix \n",
    "tf_query_matrix = query_matrix  \n",
    "\n",
    "doc_freq = np.sum(tf_matrix > 0, axis=0) \n",
    "\n",
    "numb_docs = tf_matrix.shape[0] \n",
    "idf = np.log((numb_docs + 1) / (doc_freq + 1)) + 1 \n",
    "\n",
    "tf_idf_matrix = tf_matrix.multiply(idf)\n",
    "\n",
    "tf_idf_df = pd.DataFrame(tf_idf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "tf_query_df = pd.DataFrame(tf_query_matrix.toarray(), columns=vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the matrices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_norm = normalize(tf_idf_df)\n",
    "tf_query_norm = normalize(tf_query_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_rankings(document_matrix, query_matrix, corpus_data_df):\n",
    "    #Calculate the cosine similarity between each query and document\n",
    "    cosine_similarity_matrix = query_matrix @ document_matrix.T\n",
    "    \n",
    "    #Take the top 10 documents with highest cosine similarity per query\n",
    "    top_10_results = []\n",
    "    for i in range(cosine_similarity_matrix.shape[0]):\n",
    "        top_10_indices = np.argsort(-cosine_similarity_matrix[i])[:10]\n",
    "        top_10_doc_ids = [corpus_data_df['docid'].iloc[j] for j in top_10_indices]\n",
    "        top_10_similarities = cosine_similarity_matrix[i, top_10_indices]\n",
    "        top_10_results.append(list(zip(top_10_doc_ids, top_10_similarities)))\n",
    "    \n",
    "    return top_10_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rocchio's algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method of query expansion uses pseudo-relevance feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_query(tf_idf_norm_mat, tf_query_norm_mat, corpus_data_df, alpha, beta, gamma,\n",
    "                 k=10,p=5,q=5):\n",
    "    \n",
    "    expanded_query_matrix = []\n",
    "    \n",
    "    cos_sim_mat = tf_query_norm_mat @ tf_idf_norm_mat.T\n",
    "    \n",
    "    for idx, query_vector in enumerate(tf_query_norm_mat):\n",
    "        #Gets the top 10 document indices based on cosine similarity\n",
    "        top_10_indices = np.argsort(-cos_sim_mat[idx])[:10] \n",
    "        #Considers the top p documents within the 10 to be relevant\n",
    "        top_rel_doc_vecs = [tf_idf_norm_mat[i] for i in top_10_indices[:p]]\n",
    "        #Considers the bottom q documents within the 10 be non-relevant\n",
    "        bot_non_rel_doc_vecs = [tf_idf_norm_mat[i] for i in top_10_indices[-q:]]\n",
    "        #Determines how much weight to attribute to the original query\n",
    "        norm_query_vector = [alpha * weight for weight in query_vector]\n",
    "        #Determines how much weight to give to the centroid of relevant documents\n",
    "        norm_sum_relevant = [beta*sum(x)/len(top_rel_doc_vecs) for x in zip(*top_rel_doc_vecs)]\n",
    "        #Determines how much weight to give to the centroid of non-relevant documents\n",
    "        norm_sum_non_relevant = [-gamma*sum(x)/len(bot_non_rel_doc_vecs) for x in zip(*bot_non_rel_doc_vecs)]\n",
    "        \n",
    "        modified_query_vector = [sum(x) for x in zip(norm_sum_relevant, norm_sum_non_relevant, norm_query_vector)]\n",
    "        \n",
    "        modified_query_vector = [x if x > 0 else 0 for x in modified_query_vector]\n",
    "        \n",
    "        expanded_query_matrix.append(modified_query_vector)\n",
    "    \n",
    "    return expanded_query_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ranking the documents and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below gets the initial rankings of cosine similarity between queries and documents prior to query expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_results = initial_rankings(tf_idf_norm,tf_query_norm,corpus_df)\n",
    "\n",
    "for query_index, results in enumerate(top_10_results):\n",
    "    print(f\"Query {queries_df['query_id'].iloc[query_index]}:\")\n",
    "    for doc_id, similarity in results:\n",
    "        print(f\"  Document ID: {doc_id}, Cosine Similarity: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_query = expand_query(tf_idf_norm,tf_query_norm,corpus_df,1,0.75,0.15)\n",
    "modified_query_matrix = np.array(modified_query)\n",
    "mod_query_matrix_norm = normalize(modified_query_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-ranking the documents after the query expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_top_10_doc_ids = initial_rankings(tf_idf_norm,mod_query_matrix_norm,corpus_df)\n",
    "\n",
    "for query_index, results in enumerate(new_top_10_doc_ids):\n",
    "    print(f\"Query {queries_df['query_id'].iloc[query_index]}:\")\n",
    "    for doc_id, similarity in results:\n",
    "        print(f\"  Document ID: {doc_id}, Cosine Similarity: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below looks at the accuracy and sees how for how many queries the actual positive document appeared among the top 10 ranked documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity_matrix2 = tf_query_norm @ tf_idf_norm.T\n",
    "\n",
    "correct_count = 0  # count\n",
    "for i in range(cosine_similarity_matrix2.shape[0]):\n",
    "    top_10_indices = np.argsort(-cosine_similarity_matrix2[i])[:10]  # get top 10 indices\n",
    "    top_10_doc_ids = [corpus_df['docid'].iloc[j] for j in top_10_indices]  #  get top 10 doc_ids\n",
    "    print(f\"Query {queries_df['query_id'].iloc[i]}: {queries_df['positive_docs'].iloc[i]}\")\n",
    "    print(\"Top 10 Document IDs:\", top_10_doc_ids)\n",
    "\n",
    "    # check if positive id is in top 10\n",
    "    if queries_df['positive_docs'].iloc[i] in top_10_doc_ids:\n",
    "        correct_count += 1  #  if positive id is in top 10, count + 1\n",
    "\n",
    "# calculate accuracy\n",
    "accuracy = correct_count / len(queries_df['positive_docs'])\n",
    "# print accuracy\n",
    "print()\n",
    "print()\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "disproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
