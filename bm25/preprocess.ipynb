{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess\n",
    "\n",
    "Here we just show how we preprocess our datasets,\n",
    "\n",
    "you don't need to run this actually, as we have provided all the preprocessed datasets. :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seperate the corpus\n",
    "\n",
    "First of all, we seperate the corpus into 7 parts with its own language.\n",
    "\n",
    "Because the corpus file is too large, it will almost cause memory crash every time it is completely loaded into the memory.\n",
    "\n",
    "so we get\n",
    "- fr_corpus.json\n",
    "- ko_corpus.json\n",
    "- es_corpus.json\n",
    "- en_corpus.json\n",
    "- it_corpus.json\n",
    "- de_corpus.json\n",
    "- ar_corpus.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus_file\n",
    "input_file = \"Data/corpus.json/corpus.json\"\n",
    "\n",
    "# read json file and split by language\n",
    "def split_json_by_language(input_file):\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # save data by language\n",
    "    language_data = {}\n",
    "\n",
    "    for item in data:\n",
    "        lang = item.get(\"lang\")\n",
    "        \n",
    "        # choose by language\n",
    "        if lang:\n",
    "            if lang not in language_data:\n",
    "                language_data[lang] = []\n",
    "            language_data[lang].append(item)\n",
    "    \n",
    "    # save data\n",
    "    for lang, items in language_data.items():\n",
    "        output_file = f\"{lang}_corpus.json\"\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(items, f, ensure_ascii=False, indent=4)\n",
    "        \n",
    "        print(f\"Saved {lang} language data to {output_file}\")\n",
    "\n",
    "# run\n",
    "split_json_by_language(input_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter the json\n",
    "\n",
    "Then we truncated corpus of longer content, because the front part of the document is usually more related to the topic, which will lose some accuracy, but will be more conducive to our extraction of max_features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# language list\n",
    "language_list = ['ar', 'es', 'fr', 'de', 'it', 'ko', 'en']\n",
    "\n",
    "# filter the json\n",
    "for lang in language_list:\n",
    "    input_file = f\"{lang}_corpus.json\"\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # filter data\n",
    "    # if the text is too long, only keep the first 30000 characters\n",
    "    for doc in data:\n",
    "        if len(doc['text']) > 30000:\n",
    "            doc['text'] = doc['text'][:30000]\n",
    "\n",
    "    # 将处理后的法语文档保存到新json文件\n",
    "    with open(f'{lang}_corpus_filtered.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### visualize the length of texts after filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('spanish_corpus_filtered.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# visualize the text length\n",
    "lengths = [len(doc['text']) for doc in data]\n",
    "\n",
    "plt.hist(lengths, bins=30)\n",
    "plt.xlabel('Text Length')\n",
    "plt.ylabel('Number of Documents')\n",
    "plt.title('Text Length Distribution')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "- Here we use nltk for stemming, as nltk doesn't support korean language, we use kiwipiepy for korean language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose language  \n",
    "lang = \"ko\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import FrenchStemmer, GermanStemmer, ItalianStemmer, SpanishStemmer, EnglishStemmer, ArabicStemmer\n",
    "\n",
    "from kiwipiepy import Kiwi  # for Korean\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from spacy.lang.ko.stop_words import STOP_WORDS as ko_stop\n",
    "\n",
    "\n",
    "def get_stemmer_and_stopwords(language):\n",
    "    \"\"\"return stemmer and stopwords based on language\"\"\"\n",
    "    if language == 'en':\n",
    "        return EnglishStemmer(), stopwords.words(\"english\")\n",
    "    elif language == 'fr':\n",
    "        return FrenchStemmer(), stopwords.words(\"french\")\n",
    "    elif language == 'de':\n",
    "        return GermanStemmer(), stopwords.words(\"german\")\n",
    "    elif language == 'it':\n",
    "        return ItalianStemmer(), stopwords.words(\"italian\")\n",
    "    elif language == 'es':\n",
    "        return SpanishStemmer(), stopwords.words(\"spanish\")\n",
    "    elif language == 'ar':\n",
    "        return ArabicStemmer(), stopwords.words(\"arabic\")\n",
    "    elif language == 'ko':\n",
    "        return Kiwi(), list(ko_stop)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported language: {language}\")\n",
    "\n",
    "def process_text(text, stemmer, stop_words, language):\n",
    "    \"\"\"Process and tokenize a single text based on language.\"\"\"\n",
    "    text = text.translate(str.maketrans(string.punctuation, ' ' * len(string.punctuation)))\n",
    "\n",
    "    if language != 'ko':\n",
    "        tokens = word_tokenize(text) \n",
    "        tokenized_words = [stemmer.stem(word.lower()) for word in tokens if word.lower() not in stop_words]\n",
    "    else:\n",
    "        tokens = stemmer.analyze(text)[0][0] \n",
    "        tokenized_words = [word.form for word in tokens if word.tag.startswith('N') and word.form not in stop_words] \n",
    "    \n",
    "    return tokenized_words\n",
    "\n",
    "def tokenizer(df, col_name, language, mytype):\n",
    "    \"\"\"Tokenize the column of a given dataframe based on the language.\"\"\"\n",
    "    # get stemmer and stopwords based on language\n",
    "    stemmer, stop_words = get_stemmer_and_stopwords(language)\n",
    "    \n",
    "    # create a new column with tokenized text\n",
    "    new_df = df.copy()\n",
    "    new_col_name = col_name + \"_token\"\n",
    "    new_df[new_col_name] = new_df[col_name].apply(lambda text: process_text(text, stemmer, stop_words, language))\n",
    "    \n",
    "    print(f\"Data has been successfully tokenized for language: {language}\")\n",
    "    \n",
    "    # save the new data as CSV\n",
    "    output_path = f\"Data/test/bm25_{language}_{mytype}.csv\"\n",
    "    new_df.to_csv(output_path, index=False)\n",
    "    print(f\"New data saved as CSV at {output_path}!\")\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "CORPUS_PATH = f\"{lang}_corpus_filtered.json\"\n",
    "QUERY_PATH = \"Data/dev.csv\"\n",
    "corpus = pd.read_json(CORPUS_PATH)\n",
    "query = pd.read_csv(QUERY_PATH)\n",
    "\n",
    "# Restrict the data to the wanted language defined above\n",
    "corpus_data = corpus[corpus[\"lang\"] == lang]\n",
    "query_data = query[query[\"lang\"] == lang]\n",
    "\n",
    "# Tokenization\n",
    "corpus_data = tokenizer(corpus_data , \"text\", lang, \"corpus\")\n",
    "query_data = tokenizer(query_data, \"query\", lang, \"query\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the results here\n",
    "\n",
    "We will use 'text_token' column for the following work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>docid</th>\n",
       "      <th>text</th>\n",
       "      <th>lang</th>\n",
       "      <th>text_token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>doc-es-214</td>\n",
       "      <td>Beautiful Boy: Siempre serás mi hijo\\n\\nArgume...</td>\n",
       "      <td>es</td>\n",
       "      <td>['beautiful', 'boy', 'siempr', 'hij', 'argumen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>doc-es-9159</td>\n",
       "      <td>Vautour (buque de 1797)\\n\\nCorsario francés\\n\\...</td>\n",
       "      <td>es</td>\n",
       "      <td>['vautour', 'buqu', '1797', 'corsari', 'france...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>doc-es-5214</td>\n",
       "      <td>Certificados de Capital de Desarrollo\\n\\nLos C...</td>\n",
       "      <td>es</td>\n",
       "      <td>['certific', 'capital', 'desarroll', 'certific...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>doc-es-1780</td>\n",
       "      <td>Chispazos de tradición\\n\\nCaracterísticas del ...</td>\n",
       "      <td>es</td>\n",
       "      <td>['chispaz', 'tradicion', 'caracterist', 'progr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>doc-es-9639</td>\n",
       "      <td>Jericó\\n\\nToponimia\\n\\nEn Canaán, en el moment...</td>\n",
       "      <td>es</td>\n",
       "      <td>['jeric', 'toponimi', 'canaan', 'moment', 'con...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0        docid                                               text  \\\n",
       "0           0   doc-es-214  Beautiful Boy: Siempre serás mi hijo\\n\\nArgume...   \n",
       "1           1  doc-es-9159  Vautour (buque de 1797)\\n\\nCorsario francés\\n\\...   \n",
       "2           2  doc-es-5214  Certificados de Capital de Desarrollo\\n\\nLos C...   \n",
       "3           3  doc-es-1780  Chispazos de tradición\\n\\nCaracterísticas del ...   \n",
       "4           4  doc-es-9639  Jericó\\n\\nToponimia\\n\\nEn Canaán, en el moment...   \n",
       "\n",
       "  lang                                         text_token  \n",
       "0   es  ['beautiful', 'boy', 'siempr', 'hij', 'argumen...  \n",
       "1   es  ['vautour', 'buqu', '1797', 'corsari', 'france...  \n",
       "2   es  ['certific', 'capital', 'desarroll', 'certific...  \n",
       "3   es  ['chispaz', 'tradicion', 'caracterist', 'progr...  \n",
       "4   es  ['jeric', 'toponimi', 'canaan', 'moment', 'con...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang = 'es'\n",
    "corpus_data = pd.read_csv(f\"Data/preprocess_corpus/bm25corpus_{lang}.csv\")\n",
    "query_data = pd.read_csv(f\"Data/preprocess_query/bm25query_{lang}.csv\")\n",
    "\n",
    "# check the result \n",
    "\n",
    "corpus_data.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DIS1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
