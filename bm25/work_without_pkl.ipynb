{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD\n",
    "\n",
    "suppose you have already download needed csv files.\n",
    "\n",
    "links:  https://drive.google.com/drive/folders/1zQp9VdhdXmG7_DK7O_j5CP2SEHuM6VTe?usp=drive_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "lang = \"fr\"\n",
    "# If the data has already been tokenized and saved, only need to run this cell\n",
    "corpus_file = pd.read_csv(f\"Data/test/bm25_{lang}_corpus.csv\")\n",
    "query_file = pd.read_csv(f\"Data/test/bm25_{lang}_query.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BM25 Probabilistic Language Model Implementation\n",
    "\n",
    "For this notebook, we will define our model as follow: we will use a bag-of-words retrieval function, named BM25. We will use the following scoring for a query Q (with words after tokenizations $\\{q_i\\}_{i=1, …, n}$) and a document D:\n",
    "\n",
    "$$score(Q, D) = \\sum_{i=1}^n IDF(q_i) * \\frac{f(q_i, D) \\cdot (k + 1)}{f(q_i, D) + k \\cdot (1 - b + b \\cdot \\frac{|D|}{avglength})}$$\n",
    "\n",
    "where $|D|$ = number of tokens in document D, $f(q_i, D)$ = number of times $q_i$ occurs in document D, avglength = average length of a token in the text collection.\n",
    "\n",
    "Moreover, k and b are parameters to finetune."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implemention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def avg_doc_length(corpus):\n",
    "    \"\"\"Compute the average length (in tokens) of the documents of the whole corpus\"\"\"\n",
    "    documents = corpus[\"text_token\"]\n",
    "    return np.mean([len(doc) for doc in documents])\n",
    "\n",
    "def doc_lengths(corpus):\n",
    "    documents = corpus[\"text_token\"]\n",
    "    return np.array([len(doc) for doc in documents])\n",
    "\n",
    "def scores_bm25(queries, corpus, N, max_features, doc_lengths=None, avgLength=None, k1=1.25, b=0.75):\n",
    "    vectorizer = CountVectorizer(max_features=max_features)\n",
    "    doc_term_matrix = vectorizer.fit_transform(corpus['text_token'])\n",
    "    query_term_matrix = vectorizer.transform(queries['query_token'])\n",
    "    \n",
    "    F = doc_term_matrix.toarray()\n",
    "    if doc_lengths is None:\n",
    "        doc_lengths = F.sum(axis=1)\n",
    "    if avgLength is None:\n",
    "        avg_doc_length = np.mean(doc_lengths)\n",
    "    else:\n",
    "        avg_doc_length = avgLength\n",
    "    \n",
    "    df = np.count_nonzero(F, axis=0)\n",
    "    idf = np.log(1 + (N - df + 0.5) / (df + 0.5))\n",
    "    \n",
    "    numerator = F * (k1 + 1)\n",
    "    denominator = F + k1 * (1 - b + b * (doc_lengths[:, None] / avg_doc_length))\n",
    "    F_adjusted = numerator / denominator\n",
    "    idf_times_F_adjusted = idf * F_adjusted\n",
    "    \n",
    "    BM25_scores = query_term_matrix.toarray() @ idf_times_F_adjusted.T\n",
    "\n",
    "    scores_list = [\n",
    "        {'query_id': queries['query_id'][query_idx], 'doc_id': corpus['docid'][doc_idx], 'bm25_score': score}\n",
    "        for query_idx, query_id in enumerate(queries['query_id'])\n",
    "        for doc_idx, doc_id in enumerate(corpus['docid'])\n",
    "        for score in [BM25_scores[query_idx, doc_idx]]\n",
    "    ]\n",
    "    \n",
    "    return pd.DataFrame(scores_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_acc(pos_docs, query):\n",
    "    acc = 0\n",
    "    for i, id in enumerate(query[\"query_id\"]):\n",
    "        if query[\"positive_docs\"][i] in pos_docs[id]:\n",
    "            acc += 1\n",
    "    return acc/len(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run\n",
    "\n",
    "change max_features to test for better results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 25000\n",
    "\n",
    "avg_doc_len = avg_doc_length(corpus_file)\n",
    "doc_len = doc_lengths(corpus_file)\n",
    "\n",
    "scores = scores_bm25(query_file, corpus_file, corpus_file.shape[0], max_features, doc_len, avg_doc_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get top 10 results:\n",
    "pos_docs = {}\n",
    "for i, id in enumerate(query_file[\"query_id\"]):\n",
    "    scores_id = scores[scores[\"query_id\"] == id]\n",
    "    # sort depending on the score values\n",
    "    scores_id = scores_id.sort_values(by='bm25_score', ascending=False)\n",
    "    pos_docs[id] = scores_id[\"doc_id\"][:10].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc(pos_docs, query_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import joblib\n",
    "\n",
    "# calculate the average document length\n",
    "def _avg_doc_length(corpus):\n",
    "    documents = corpus[\"text_token\"]\n",
    "    return np.mean([len(doc) for doc in documents])\n",
    "\n",
    "# calculate bm25 scores\n",
    "def save_bm25_corpus(corpus,lang, max_features=25000, k1=1.25, b=0.75):\n",
    "    vectorizer = CountVectorizer(max_features=max_features)\n",
    "    doc_term_matrix = vectorizer.fit_transform(corpus['text_token'])\n",
    "    \n",
    "    # transform the doc_term_matrix to an array\n",
    "    F = doc_term_matrix.toarray()\n",
    "    doc_lengths = F.sum(axis=1)\n",
    "    avg_doc_length = _avg_doc_length(corpus)\n",
    "    \n",
    "    # get idf\n",
    "    N = len(corpus)\n",
    "    df = np.count_nonzero(F, axis=0)\n",
    "    idf = np.log(1 + (N - df + 0.5) / (df + 0.5))\n",
    "    \n",
    "    # calculate the bm25 score\n",
    "    numerator = F * (k1 + 1)\n",
    "    denominator = F + k1 * (1 - b + b * (doc_lengths[:, None] / avg_doc_length))\n",
    "    F_adjusted = numerator / denominator\n",
    "    idf_times_F_adjusted = idf * F_adjusted\n",
    "    \n",
    "    # save the bm25 corpus\n",
    "    doc_ids = corpus['docid'].tolist()\n",
    "    joblib.dump((idf_times_F_adjusted, vectorizer, doc_ids), f\"bm25_corpus_{lang}.pkl\")\n",
    "    print(\"BM25 corpus saved successfully!\")\n",
    "\n",
    "lang = \"es\"\n",
    "corpus = pd.read_csv(f'Data/preprocess_corpus/bm25corpus_{lang}.csv')\n",
    "save_bm25_corpus(corpus, lang)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test\n",
    "\n",
    "get results for test.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = pd.read_csv(f\"Data/preprocess_test/bm25query_{lang}_test.csv\")\n",
    "\n",
    "avg_doc_len = avg_doc_length(corpus_file)\n",
    "doc_len = doc_lengths(corpus_file)\n",
    "\n",
    "scores= scores_bm25(test_file, corpus_file, corpus_file.shape[0], 35000, doc_len, avg_doc_len)\n",
    "\n",
    "\n",
    "# get top 10 results:\n",
    "pos_docs = {}\n",
    "for i, id in enumerate(test_file[\"query_id\"]):\n",
    "    scores_id = scores[scores[\"query_id\"] == id]\n",
    "    # sort depending on the score values\n",
    "    scores_id = scores_id.sort_values(by='bm25_score', ascending=False)\n",
    "    pos_docs[id] = scores_id[\"doc_id\"][:10].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the results as csv\n",
    "\n",
    "with open(f\"Data/bm25_results_{lang}_new.csv\", \"w\") as f:\n",
    "    f.write(\"id,docids\\n\")\n",
    "    for key in pos_docs.keys():\n",
    "        f.write(f\"{key},{' '.join(pos_docs[key])}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "disproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
